JSON_DIR = "/Users/danny.paleyev/dl2_bhw1/TinyStories_all_data"
TXT_DIR = "/Users/danny.paleyev/dl2_bhw1/TinyStories_txt"
DATASET_DIR = "/Users/danny.paleyev/dl2_bhw1/tokenized"
VAL_TXT = "/Users/danny.paleyev/dl2_bhw1/TinyStoriesV3-GPT4-valid.txt"

TOKENIZER_PATH = "/Users/danny.paleyev/dl2_bhw1/tokenizer.model"
VOCAB_SIZE = 16000
MAX_SEQ_LEN = 256

LEARNING_RATE = 3e-4
WEIGHT_DECAY = 1e-5

BATCH_SIZE = 64
EPOCHS = 25

EMBED_DIM = 512
N_HEADS = 4
HIDDEN_DIM = 512
NUM_LAYERS = 2
