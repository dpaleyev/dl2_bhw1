JSON_DIR = "/notebooks/TinyStories_all_data"
TXT_DIR = "/notebooks/TinyStories_txt"
DATASET_DIR = "/notebooks/tokenized"
VAL_TXT = "/notebooks/TinyStoriesV3-GPT4-valid.txt"

TOKENIZER_PATH = "/notebooks/tokenizer.model"
RUN_NAME = "test"

VOCAB_SIZE = 16000
MAX_SEQ_LEN = 256

LEARNING_RATE = 3e-4
WEIGHT_DECAY = 1e-5

BATCH_SIZE = 64
EPOCHS = 25

EMBED_DIM = 512
N_HEADS = 4
HIDDEN_DIM = 512
NUM_LAYERS = 2
